global:
  repository: registry.sports-cloud.com:5000

sc-monitoring:
  default-http-backend:
    image:
      repository: gcr.io/google_containers
      tag: defaultbackend:1.0
      pullPolicy: IfNotPresent   
    deployment:
      resources:
        minCpu: 10m
        maxCpu: 10m
        minMem: 20Mi
        maxMem: 20Mi
      containerPort: 8080
      replicas: 1
      terminationGracePeriodSeconds: 60
      livenessPath: "/healthz"
      livenessProbeInitDelay: 30
      livenessProbeTimeoutSeconds: 5
      livenessProbePort: 8080  
    service:
      name: default-http-backend
      type: NodePort
      externalPort: 8080
      internalPort: 80
  logrotate:
    image:
      tag: sc-monitoring:0.0.1-alpha.1
      pullPolicy: IfNotPresent   
    deployment:
      resources:
        minCpu: 10m
        maxCpu: 10m
        minMem: 20Mi
        maxMem: 20Mi
      command:
      - "/scripts/execute-logrotate.sh"
      - "1800"     
    service:
      name: default-http-backend
      type: NodePort
      externalPort: 8080
      internalPort: 80
  prometheus:
    rbac:
      create: false

    alertmanager:
      ## If false, alertmanager will not be installed
      ##
      enabled: true

      # Defines the serviceAccountName to use when `rbac.create=false`
      serviceAccountName: default

      ## alertmanager container name
      ##
      name: alertmanager

      ## alertmanager container image
      ##
      image:
        repository: prom/alertmanager
        tag: v0.14.0
        pullPolicy: IfNotPresent

      ## Additional alertmanager container arguments
      ##
      extraArgs: {}

      ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
      ## so that the various internal URLs are still able to access as they are in the default case.
      ## (Optional)
      prefixURL: ""

      ## External URL which can access alertmanager
      ## Maybe same with Ingress host name
      baseURL: "/"

      ## Additional alertmanager container environment variable
      ## For instance to add a http_proxy
      ##
      extraEnv: {}

      ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.alertmanager.configMapOverrideName}}
      ## Defining configMapOverrideName will cause templates/alertmanager-configmap.yaml
      ## to NOT generate a ConfigMap resource
      ##
      configMapOverrideName: ""

      ingress:
        ## If true, alertmanager Ingress will be created
        ##
        enabled: false

        ## alertmanager Ingress annotations
        ##
        annotations: {}
        #   kubernetes.io/ingress.class: nginx
        #   kubernetes.io/tls-acme: 'true'

        ## alertmanager Ingress hostnames with optional path
        ## Must be provided if Ingress is enabled
        ##
        hosts: []
        #   - alertmanager.domain.com
        #   - domain.com/alertmanager

        ## alertmanager Ingress TLS configuration
        ## Secrets must be manually created in the namespace
        ##
        tls: []
        #   - secretName: prometheus-alerts-tls
        #     hosts:
        #       - alertmanager.domain.com

      ## Alertmanager Deployment Strategy type
      # strategy:
      #   type: Recreate

      ## Node tolerations for alertmanager scheduling to nodes with taints
      ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      ##
      tolerations: []
        # - key: "key"
        #   operator: "Equal|Exists"
        #   value: "value"
        #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

      ## Node labels for alertmanager pod assignment
      ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

      persistentVolume:
        ## If true, alertmanager will create/use a Persistent Volume Claim
        ## If false, use emptyDir
        ##
        enabled: true

        ## alertmanager data Persistent Volume access modes
        ## Must match those of existing PV or dynamic provisioner
        ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
        ##
        accessModes:
          - ReadWriteOnce

        ## alertmanager data Persistent Volume Claim annotations
        ##
        annotations: {}

        ## alertmanager data Persistent Volume existing claim name
        ## Requires alertmanager.persistentVolume.enabled: true
        ## If defined, PVC must be created manually before volume will be bound
        existingClaim: ""

        ## alertmanager data Persistent Volume mount root path
        ##
        mountPath: /data

        ## alertmanager data Persistent Volume size
        ##
        size: 2Gi

        ## alertmanager data Persistent Volume Storage Class
        ## If defined, storageClassName: <storageClass>
        ## If set to "-", storageClassName: "", which disables dynamic provisioning
        ## If undefined (the default) or set to null, no storageClassName spec is
        ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
        ##   GKE, AWS & OpenStack)
        ##
        # storageClass: "-"

        ## Subdirectory of alertmanager data Persistent Volume to mount
        ## Useful if the volume's root directory is not empty
        ##
        subPath: ""

      ## Annotations to be added to alertmanager pods
      ##
      podAnnotations: {}

      replicaCount: 1

      ## alertmanager resource requests and limits
      ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: 
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 500m
          memory: 32Mi

      service:
        annotations: {}
        labels: {}
        clusterIP: ""

        ## Enabling peer mesh service end points for enabling the HA alert manager
        ## Ref: https://github.com/prometheus/alertmanager/blob/master/README.md
        # enableMeshPeer : true

        ## List of IP addresses at which the alertmanager service is available
        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
        ##
        externalIPs: []

        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        servicePort: 80
        # nodePort: 30000
        type: ClusterIP

    ## Monitors ConfigMap changes and POSTs to a URL
    ## Ref: https://github.com/jimmidyson/configmap-reload
    ##
    configmapReload:
      ## configmap-reload container name
      ##
      name: configmap-reload

      ## configmap-reload container image
      ##
      image:
        repository: jimmidyson/configmap-reload
        tag: v0.1
        pullPolicy: IfNotPresent

      ## Additional configmap-reload container arguments
      ##
      extraArgs: {}

      ## Additional configmap-reload mounts
      ##
      extraConfigmapMounts: []
        # - name: prometheus-alerts
        #   mountPath: /etc/alerts.d
        #   configMap: prometheus-alerts
        #   readOnly: true


      ## configmap-reload resource requests and limits
      ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: {}

    initChownData:
      ## If false, data ownership will not be reset at startup
      ## This allows the prometheus-server to be run with an arbitrary user
      ##
      enabled: true

      ## initChownData container name
      ##
      name: init-chown-data

      ## initChownData container image
      ##
      image:
        repository: busybox
        tag: latest
        pullPolicy: IfNotPresent

      ## initChownData resource requests and limits
      ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: {}

    kubeStateMetrics:
      ## If false, kube-state-metrics will not be installed
      ##
      enabled: true

      # Defines the serviceAccountName to use when `rbac.create=false`
      serviceAccountName: default

      ## kube-state-metrics container name
      ##
      name: kube-state-metrics

      ## kube-state-metrics container image
      ##
      image:
        repository: k8s.gcr.io/kube-state-metrics
        tag: v1.2.0
        pullPolicy: IfNotPresent

      ## kube-state-metrics container arguments
      ##
      args: {}

      ## Node tolerations for kube-state-metrics scheduling to nodes with taints
      ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      ##
      tolerations: []
        # - key: "key"
        #   operator: "Equal|Exists"
        #   value: "value"
        #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

      ## Node labels for kube-state-metrics pod assignment
      ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

      ## Annotations to be added to kube-state-metrics pods
      ##
      podAnnotations: {}

      replicaCount: 1

      ## kube-state-metrics resource requests and limits
      ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: {}
        # limits:
        #   cpu: 10m
        #   memory: 16Mi
        # requests:
        #   cpu: 10m
        #   memory: 16Mi

      service:
        annotations:
          prometheus.io/scrape: "true"
        labels: {}

        clusterIP: None

        ## List of IP addresses at which the kube-state-metrics service is available
        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
        ##
        externalIPs: []

        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        servicePort: 80
        type: ClusterIP

    nodeExporter:
      ## If false, node-exporter will not be installed
      ##
      enabled: true

      # Defines the serviceAccountName to use when `rbac.create=false`
      serviceAccountName: default

      ## node-exporter container name
      ##
      name: node-exporter

      ## node-exporter container image
      ##
      image:
        repository: prom/node-exporter
        tag: v0.15.2
        pullPolicy: IfNotPresent

      ## Custom Update Strategy
      ##
      updateStrategy:
        type: OnDelete

      ## Additional node-exporter container arguments
      ##
      extraArgs: {}

      ## Additional node-exporter hostPath mounts
      ##
      extraHostPathMounts: []
        # - name: textfile-dir
        #   mountPath: /srv/txt_collector
        #   hostPath: /var/lib/node-exporter
        #   readOnly: true

      extraConfigmapMounts: []
        # - name: certs-configmap
        #   mountPath: /prometheus
        #   configMap: certs-configmap
        #   readOnly: true

      ## Node tolerations for node-exporter scheduling to nodes with taints
      ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      ##
      tolerations: []
        # - key: "key"
        #   operator: "Equal|Exists"
        #   value: "value"
        #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

      ## Node labels for node-exporter pod assignment
      ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

      ## Annotations to be added to node-exporter pods
      ##
      podAnnotations: {}

      ## node-exporter resource limits & requests
      ## Ref: https://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: {}
        # limits:
        #   cpu: 200m
        #   memory: 50Mi
        # requests:
        #   cpu: 100m
        #   memory: 30Mi

      ## Security context to be added to node-exporter pods
      ##
      securityContext: {}
        # runAsUser: 0

      service:
        annotations:
          prometheus.io/scrape: "true"
        labels: {}

        clusterIP: None

        ## List of IP addresses at which the node-exporter service is available
        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
        ##
        externalIPs: []

        hostPort: 9100
        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        servicePort: 9100
        type: ClusterIP

    server:
      ## Prometheus server container name
      ##
      name: server

      # Defines the serviceAccountName to use when `rbac.create=false`
      serviceAccountName: default

      ## Prometheus server container image
      ##
      image:
        repository: prom/prometheus
        tag: v2.2.1
        pullPolicy: IfNotPresent

      ## The URL prefix at which the container can be accessed. Useful in the case the '-web.external-url' includes a slug
      ## so that the various internal URLs are still able to access as they are in the default case.
      ## (Optional)
      prefixURL: ""

      ## External URL which can access alertmanager
      ## Maybe same with Ingress host name
      baseURL: ""

      ## Additional Prometheus server container arguments
      ##
      extraArgs: {}

      ## Additional Prometheus server hostPath mounts
      ##
      extraHostPathMounts: []
        # - name: certs-dir
        #   mountPath: /etc/kubernetes/certs
        #   hostPath: /etc/kubernetes/certs
        #   readOnly: true

      extraConfigmapMounts: []
        # - name: certs-configmap
        #   mountPath: /prometheus
        #   configMap: certs-configmap
        #   readOnly: true

      ## Additional Prometheus server Secret mounts
      # Defines additional mounts with secrets. Secrets must be manually created in the namespace.
      extraSecretMounts: []
        # - name: secret-files
        #   mountPath: /etc/secrets
        #   secretName: prom-secret-files
        #   readOnly: true

      ## ConfigMap override where fullname is {{.Release.Name}}-{{.Values.server.configMapOverrideName}}
      ## Defining configMapOverrideName will cause templates/server-configmap.yaml
      ## to NOT generate a ConfigMap resource
      ##
      configMapOverrideName: ""

      ingress:
        ## If true, Prometheus server Ingress will be created
        ##
        enabled: false

        ## Prometheus server Ingress annotations
        ##
        annotations: {}
        #   kubernetes.io/ingress.class: nginx
        #   kubernetes.io/tls-acme: 'true'

        ## Prometheus server Ingress hostnames with optinal path
        ## Must be provided if Ingress is enabled
        ##
        hosts: []
        #   - prometheus.domain.com
        #   - domain.com/prometheus

        ## Prometheus server Ingress TLS configuration
        ## Secrets must be manually created in the namespace
        ##
        tls: []
        #   - secretName: prometheus-server-tls
        #     hosts:
        #       - prometheus.domain.com

      ## Server Deployment Strategy type
      # strategy:
      #   type: Recreate

      ## Node tolerations for server scheduling to nodes with taints
      ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      ##
      tolerations: []
        # - key: "key"
        #   operator: "Equal|Exists"
        #   value: "value"
        #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

      ## Node labels for Prometheus server pod assignment
      ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

      persistentVolume:
        ## If true, Prometheus server will create/use a Persistent Volume Claim
        ## If false, use emptyDir
        ##
        enabled: true

        ## Prometheus server data Persistent Volume access modes
        ## Must match those of existing PV or dynamic provisioner
        ## Ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
        ##
        accessModes:
          - ReadWriteOnce

        ## Prometheus server data Persistent Volume annotations
        ##
        annotations: {}

        ## Prometheus server data Persistent Volume existing claim name
        ## Requires server.persistentVolume.enabled: true
        ## If defined, PVC must be created manually before volume will be bound
        existingClaim: ""

        ## Prometheus server data Persistent Volume mount root path
        ##
        mountPath: /data

        ## Prometheus server data Persistent Volume size
        ##
        size: 8Gi

        ## Prometheus server data Persistent Volume Storage Class
        ## If defined, storageClassName: <storageClass>
        ## If set to "-", storageClassName: "", which disables dynamic provisioning
        ## If undefined (the default) or set to null, no storageClassName spec is
        ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
        ##   GKE, AWS & OpenStack)
        ##
        # storageClass: "-"

        ## Subdirectory of Prometheus server data Persistent Volume to mount
        ## Useful if the volume's root directory is not empty
        ##
        subPath: ""

      ## Annotations to be added to Prometheus server pods
      ##
      podAnnotations: {}
        # iam.amazonaws.com/role: prometheus

      replicaCount: 1

      ## Prometheus server resource requests and limits
      ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: {}
        # limits:
        #   cpu: 500m
        #   memory: 512Mi
        # requests:
        #   cpu: 500m
        #   memory: 512Mi

      service:
        annotations: {}
        labels: {}
        clusterIP: ""

        ## List of IP addresses at which the Prometheus server service is available
        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
        ##
        externalIPs: []

        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        servicePort: 80
        type: NodePort

      ## Prometheus server pod termination grace period
      ##
      terminationGracePeriodSeconds: 300

      ## Prometheus data retention period (i.e 360h)
      ##
      retention: ""

    pushgateway:
      ## If false, pushgateway will not be installed
      ##
      enabled: true

      ## pushgateway container name
      ##
      name: pushgateway

      ## pushgateway container image
      ##
      image:
        repository: prom/pushgateway
        tag: v0.4.0
        pullPolicy: IfNotPresent

      ## Additional pushgateway container arguments
      ##
      extraArgs: {}

      ingress:
        ## If true, pushgateway Ingress will be created
        ##
        enabled: false

        ## pushgateway Ingress annotations
        ##
        annotations: {}
        #   kubernetes.io/ingress.class: nginx
        #   kubernetes.io/tls-acme: 'true'

        ## pushgateway Ingress hostnames with optinal path
        ## Must be provided if Ingress is enabled
        ##
        hosts: []
        #   - pushgateway.domain.com
        #   - domain.com/pushgateway

        ## pushgateway Ingress TLS configuration
        ## Secrets must be manually created in the namespace
        ##
        tls: []
        #   - secretName: prometheus-alerts-tls
        #     hosts:
        #       - pushgateway.domain.com

      ## Node tolerations for pushgateway scheduling to nodes with taints
      ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
      ##
      tolerations: []
        # - key: "key"
        #   operator: "Equal|Exists"
        #   value: "value"
        #   effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

      ## Node labels for pushgateway pod assignment
      ## Ref: https://kubernetes.io/docs/user-guide/node-selection/
      ##
      nodeSelector: {}

      ## Annotations to be added to pushgateway pods
      ##
      podAnnotations: {}

      replicaCount: 1

      ## pushgateway resource requests and limits
      ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
      ##
      resources: {}
        # limits:
        #   cpu: 10m
        #   memory: 32Mi
        # requests:
        #   cpu: 10m
        #   memory: 32Mi

      service:
        annotations:
          prometheus.io/probe: pushgateway
        labels: {}
        clusterIP: ""

        ## List of IP addresses at which the pushgateway service is available
        ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
        ##
        externalIPs: []

        loadBalancerIP: ""
        loadBalancerSourceRanges: []
        servicePort: 9091
        type: ClusterIP

    ## alertmanager ConfigMap entries
    ##
    alertmanagerFiles:
      alertmanager.yml:
        global:
          # slack_api_url: ''

        receivers:
          - name: default-receiver
            # slack_configs:
            #  - channel: '@you'
            #    send_resolved: true

        route:
          group_wait: 10s
          group_interval: 5m
          receiver: default-receiver
          repeat_interval: 3h

    ## Prometheus server ConfigMap entries
    ##
    serverFiles:
      alerts: {}
      rules: {}

      prometheus.yml:
        rule_files:
          - /etc/config/rules
          - /etc/config/alerts

        scrape_configs:
          - job_name: prometheus
            static_configs:
              - targets:
                - localhost:9090

          # A scrape configuration for running Prometheus on a Kubernetes cluster.
          # This uses separate scrape configs for cluster components (i.e. API server, node)
          # and services to allow each to use different authentication configs.
          #
          # Kubernetes labels will be added as Prometheus labels on metrics via the
          # `labelmap` relabeling action.

          # Scrape config for API servers.
          #
          # Kubernetes exposes API servers as endpoints to the default/kubernetes
          # service so this uses `endpoints` role and uses relabelling to only keep
          # the endpoints associated with the default/kubernetes service using the
          # default named port `https`. This works for single API server deployments as
          # well as HA API server deployments.
          - job_name: 'kubernetes-apiservers'

            kubernetes_sd_configs:
              - role: endpoints

            # Default to scraping over https. If required, just disable this or change to
            # `http`.
            scheme: https

            # This TLS & bearer token file config is used to connect to the actual scrape
            # endpoints for cluster components. This is separate to discovery auth
            # configuration because discovery & scraping are two separate concerns in
            # Prometheus. The discovery auth config is automatic if Prometheus runs inside
            # the cluster. Otherwise, more config options have to be provided within the
            # <kubernetes_sd_config>.
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              # If your node certificates are self-signed or use a different CA to the
              # master CA, then disable certificate verification below. Note that
              # certificate verification is an integral part of a secure infrastructure
              # so this should only be disabled in a controlled environment. You can
              # disable certificate verification by uncommenting the line below.
              #
              insecure_skip_verify: true
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

            # Keep only the default/kubernetes service endpoints for the https port. This
            # will add targets for each API server which Kubernetes adds an endpoint to
            # the default/kubernetes service.
            relabel_configs:
              - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
                action: keep
                regex: default;kubernetes;https

          - job_name: 'kubernetes-nodes'

            # Default to scraping over https. If required, just disable this or change to
            # `http`.
            scheme: https

            # This TLS & bearer token file config is used to connect to the actual scrape
            # endpoints for cluster components. This is separate to discovery auth
            # configuration because discovery & scraping are two separate concerns in
            # Prometheus. The discovery auth config is automatic if Prometheus runs inside
            # the cluster. Otherwise, more config options have to be provided within the
            # <kubernetes_sd_config>.
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              # If your node certificates are self-signed or use a different CA to the
              # master CA, then disable certificate verification below. Note that
              # certificate verification is an integral part of a secure infrastructure
              # so this should only be disabled in a controlled environment. You can
              # disable certificate verification by uncommenting the line below.
              #
              insecure_skip_verify: true
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

            kubernetes_sd_configs:
              - role: node

            relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)
              - target_label: __address__
                replacement: kubernetes.default.svc:443
              - source_labels: [__meta_kubernetes_node_name]
                regex: (.+)
                target_label: __metrics_path__
                replacement: /api/v1/nodes/${1}/proxy/metrics


          - job_name: 'kubernetes-nodes-cadvisor'

            # Default to scraping over https. If required, just disable this or change to
            # `http`.
            scheme: https

            # This TLS & bearer token file config is used to connect to the actual scrape
            # endpoints for cluster components. This is separate to discovery auth
            # configuration because discovery & scraping are two separate concerns in
            # Prometheus. The discovery auth config is automatic if Prometheus runs inside
            # the cluster. Otherwise, more config options have to be provided within the
            # <kubernetes_sd_config>.
            tls_config:
              ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
              # If your node certificates are self-signed or use a different CA to the
              # master CA, then disable certificate verification below. Note that
              # certificate verification is an integral part of a secure infrastructure
              # so this should only be disabled in a controlled environment. You can
              # disable certificate verification by uncommenting the line below.
              #
              insecure_skip_verify: true
            bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

            kubernetes_sd_configs:
              - role: node

            # This configuration will work only on kubelet 1.7.3+
            # As the scrape endpoints for cAdvisor have changed
            # if you are using older version you need to change the replacement to
            # replacement: /api/v1/nodes/${1}:4194/proxy/metrics
            # more info here https://github.com/coreos/prometheus-operator/issues/633
            relabel_configs:
              - action: labelmap
                regex: __meta_kubernetes_node_label_(.+)
              - target_label: __address__
                replacement: kubernetes.default.svc:443
              - source_labels: [__meta_kubernetes_node_name]
                regex: (.+)
                target_label: __metrics_path__
                replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
            metric_relabel_configs:
              - action: replace
                source_labels: [id]
                regex: '^/machine\.slice/machine-rkt\\x2d([^\\]+)\\.+/([^/]+)\.service$'
                target_label: rkt_container_name
                replacement: '${2}-${1}'
              - action: replace
                source_labels: [id]
                regex: '^/system\.slice/(.+)\.service$'
                target_label: systemd_service_name
                replacement: '${1}'
            
          # Scrape config for service endpoints.
          #
          # The relabeling allows the actual service scrape endpoint to be configured
          # via the following annotations:
          #
          # * `prometheus.io/scrape`: Only scrape services that have a value of `true`
          # * `prometheus.io/scheme`: If the metrics endpoint is secured then you will need
          # to set this to `https` & most likely set the `tls_config` of the scrape config.
          # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
          # * `prometheus.io/port`: If the metrics are exposed on a different port to the
          # service then set this appropriately.
          - job_name: 'kubernetes-service-endpoints'

            kubernetes_sd_configs:
              - role: endpoints

            relabel_configs:
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
                action: keep
                regex: true
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
                action: replace
                target_label: __scheme__
                regex: (https?)
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
                action: replace
                target_label: __address__
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_service_name]
                action: replace
                target_label: kubernetes_name

          - job_name: 'prometheus-pushgateway'
            honor_labels: true

            kubernetes_sd_configs:
              - role: service

            relabel_configs:
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
                action: keep
                regex: pushgateway

          # Example scrape config for probing services via the Blackbox Exporter.
          #
          # The relabeling allows the actual service scrape endpoint to be configured
          # via the following annotations:
          #
          # * `prometheus.io/probe`: Only probe services that have a value of `true`
          - job_name: 'kubernetes-services'

            metrics_path: /probe
            params:
              module: [http_2xx]

            kubernetes_sd_configs:
              - role: service

            relabel_configs:
              - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_probe]
                action: keep
                regex: true
              - source_labels: [__address__]
                target_label: __param_target
              - target_label: __address__
                replacement: blackbox
              - source_labels: [__param_target]
                target_label: instance
              - action: labelmap
                regex: __meta_kubernetes_service_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_service_name]
                target_label: kubernetes_name

          # Example scrape config for pods
          #
          # The relabeling allows the actual pod scrape endpoint to be configured via the
          # following annotations:
          #
          # * `prometheus.io/scrape`: Only scrape pods that have a value of `true`
          # * `prometheus.io/path`: If the metrics path is not `/metrics` override this.
          # * `prometheus.io/port`: Scrape the pod on the indicated port instead of the default of `9102`.
          - job_name: 'kubernetes-pods'

            kubernetes_sd_configs:
              - role: pod

            relabel_configs:
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                action: keep
                regex: true
              - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                action: replace
                target_label: __metrics_path__
                regex: (.+)
              - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                action: replace
                regex: ([^:]+)(?::\d+)?;(\d+)
                replacement: $1:$2
                target_label: __address__
              - action: labelmap
                regex: __meta_kubernetes_pod_label_(.+)
              - source_labels: [__meta_kubernetes_namespace]
                action: replace
                target_label: kubernetes_namespace
              - source_labels: [__meta_kubernetes_pod_name]
                action: replace
                target_label: kubernetes_pod_name

    networkPolicy:
      ## Enable creation of NetworkPolicy resources.
      ##
      enabled: false

  grafana:
    replicas: 1

    image:
      tag: sc-grafana:0.0.1-alpha.1
      pullPolicy: IfNotPresent

    downloadDashboardsImage:
      repository: appropriate/curl
      tag: latest
      pullPolicy: IfNotPresent
    ## Expose the grafana service to be accessed from outside the cluster (LoadBalancer service).
    ## or access it from within the cluster (ClusterIP service). Set the service type and the port to serve it.
    ## ref: http://kubernetes.io/docs/user-guide/services/
    ##
    service:
      type: NodePort
      port: 80
      annotations: {}

    ingress:
      enabled: false
      annotations: {}
        # kubernetes.io/ingress.class: nginx
        # kubernetes.io/tls-acme: "true"
      path: /
      hosts:
        - chart-example.local
      tls: []
      #  - secretName: chart-example-tls
      #    hosts:
      #      - chart-example.local

    resources:
      limits:
        cpu: 500m
        memory: 1Gi
      requests:
        cpu: 500m
        memory: 1Gi

    ## Node labels for pod assignment
    ## ref: https://kubernetes.io/docs/user-guide/node-selection/
    #
    nodeSelector: {}

    ## Tolerations for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
    ##
    tolerations: []

    ## Affinity for pod assignment
    ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
    ##
    affinity: {}

    ## Enable persistence using Persistent Volume Claims
    ## ref: http://kubernetes.io/docs/user-guide/persistent-volumes/
    ##
    persistence:
      enabled: true
      storageClassName: default
      accessModes:
      - ReadWriteOnce
      size: 10Gi
      # annotations: {}
      # subPath: ""
      # existingClaim:

    adminUser: admin
    # adminPassword: strongpassword


    ## Extra enviornment variables that will be pass onto deployment pods
    env:
      GF_AWS_PROFILES: "default"
      GF_AWS_default_REGION: "us-east-2"
      GF_AWS_default_ACCESS_KEY_ID: "AKIAI2L3MWG32WAGY5HQ"
      GF_AWS_default_SECRET_ACCESS_KEY: "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"

    # Pass the plugins you want installed as a comma separated list.
    # plugins: "digrich-bubblechart-panel,grafana-clock-panel"
    plugins: ""

    ## Configure grafana datasources
    ## ref: http://docs.grafana.org/administration/provisioning/#datasources
    ##
    datasources:
      prometheusName: Prometheus
      cloudwatchName: CloudWatch
      elasticSearchName: Elasticsearch

    ## Configure grafana dashboard providers
    ## ref: http://docs.grafana.org/administration/provisioning/#dashboards
    ##
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
        - name: 'default'
          orgId: 1
          folder: ''
          type: file
          disableDeletion: false
          editable: true
          options:
            path: /var/lib/grafana/dashboards

    ## Configure grafana dashboard to import
    ## NOTE: To use dashboards you must also enable/configure dashboardProviders
    ## ref: https://grafana.com/dashboards
    ##
    dashboards: {}
    #  some-dashboard:
    #    json: |
    #      $RAW_JSON
    #  prometheus-stats:
    #    gnetId: 2
    #    revision: 2
    #    datasource: Prometheus

    ## Grafana's primary configuration
    ## NOTE: values in map will be converted to ini format
    ## ref: http://docs.grafana.org/installation/configuration/
    ##
    grafana.ini:
      paths:
        data: /var/lib/grafana/data
        logs: /var/log/grafana
        plugins: /var/lib/grafana/plugins
      analytics:
        check_for_updates: true
      log:
        mode: console
      grafana_net:
        url: https://grafana.net

sc-services:
  deployment:
    terminationGracePeriodSeconds: 30
  image:
    repository: quay.io/coreos
    tag: alb-ingress-controller:1.0-alpha.7
    pullPolicy: Always
    env:
      AWS_REGION: "us-east-2"
      CLUSTER_NAME: "sports-cloud.k8s.local"
      AWS_ACCESS_KEY_ID: "AKIAI2L3MWG32WAGY5HQ"
      AWS_SECRET_ACCESS_KEY: "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"
      AWS_DEBUG: "false"
  data:
    indexing:
      elasticsearch:
        image:
            tag: sc-elasticsearch:0.0.1-alpha.8
            pullPolicy: Always
        pdb:
          master:
            maxUnavailable: 1
          data:
            maxUnavailable: 1
        services:
          data:
            internalPort: 9300
            role: data
            portName: transport
            clusterIP: None
          client:
            internalPort: 9200
            role: client
            portName: http
          discovery:
            internalPort: 9300
            role: master
            portName: transport
        deployment:
          curator:
            image: 
              repository:  quay.io/pires
              tag: docker-elasticsearch-curator:5.4.1
          master:
            role: master
            replicas: 3
            resources:
              minCpu: 500m
              maxCpu: 2
              minMem: 3Gi
              maxMem: 3.5Gi          
            livenessProbe:
              port: transport
            ports:
              transport: 9300
            env:
              NETWORK_HOST: "_eth0_"
              NUMBER_OF_MASTERS: "2"
              NODE_MASTER: "true"
              NODE_INGEST: "false"
              NODE_DATA: "false"
              HTTP_ENABLE: "false"
              ES_JAVA_OPTS: "-Xms1g -Xmx1g"
          client:
            role: client
            replicas: 2
            resources:
              minCpu: 500m
              maxCpu: 2
              minMem: 3G
              maxMem: 3.5Gi               
            livenessProbe:
              port: transport
            readinessProbe:
              port: http
              initialDelaySeconds: 20
              timeoutSeconds: 5
            ports:
              transport: 9300
              http: 9200
            env:
              NETWORK_HOST: "_eth0_"
              NODE_MASTER: "false"
              NODE_DATA: "false"
              HTTP_ENABLE: "true"
              ES_JAVA_OPTS: "-Xms1g -Xmx1g"
          data:
            role: data
            replicas: 3
            resources:
              minCpu: 500m
              maxCpu: 2
              minMem: 3Gi
              maxMem: 3.5Gi
            livenessProbe: 
              initialDelaySeconds: 20
              periodSeconds: 10
              port: transport
            storage: 60G
            ports:
              transport: 9300
            env:
              NETWORK_HOST: "_eth0_" 
              NODE_MASTER: "false"
              NODE_INGEST: "false"
              HTTP_ENABLE: "true"
              ES_JAVA_OPTS: "-Xms2g -Xmx2g"
  offline:
    zookeeper:
      image:
        repository: gcr.io/google_containers
        tag: kubernetes-zookeeper:1.0-3.4.10
        pullPolicy: Always
      pdb:
        maxUnavailable: 1
      deployment:
        replicas: 3
        resources:
          minCpu: "0.5"
          minMem: "1Gi"
        ports:
          client: 2181
          server: 2888
          leader-election: 3888   
        volumes:
          storage: 10Gi
        readinessProbe:
          initialDelaySeconds: 10
          timeoutSeconds: 5
        livenessProbe:
          initialDelaySeconds: 10
          timeoutSeconds: 5
      serviceCs:
        ports: 
          client: 2181
      serviceHs:
        clusterIP: None
        ports:
          server: 2888
          leader-election: 3888
      ## End of zookeeper config. All other dependencies will follow after this . 
      ## This is done intentionally as zookeepr is the parent of all  other components in offline 
      batch-processing:
        spark-master:
          image:
            tag: spark-master:0.0.1-alpha.3
            pullPolicy: IfNotPresent  
          deployment:
            replicas: 2
            terminationGracePeriodSeconds: 10
            command:
            - "/scripts/start-master"
            ports: ["7077","8080"]
            resources:
                minCpu: "100m"
            livenessProbe:
              port: 8080
              initialDelaySeconds: 120
              periodSeconds: 20
              timeoutSeconds: 10
          service:
              name: spark-master
              clusterIP: None
              externalPort: 7077
              internalPort: 7077
        spark-worker:
          image:
            tag: spark-worker:0.0.1-alpha.14
            pullPolicy: IfNotPresent 
            env:
              SPARK_WORKER_OPTS: "-Dspark.worker.cleanup.enabled=true -Dspark.worker.cleanup.appDataTtl=259200" 
          deployment:
            livenessProbe:
              initialDelaySeconds: 120
              periodSeconds: 20
              timeoutSeconds: 20
            resources:
              cpu: 100m
            containerPort: 8081
            replicas: 3
            command: 
            - "/scripts/start-worker"
      streaming-platform:
        ## Represents all streaming data related paltform. The one platform we chose is confluent
        confluent:
          ## Kafka deployment 
          kafka:
            # ------------------------------------------------------------------------------
            # Kafka:
            # ------------------------------------------------------------------------------

            ## The StatefulSet installs 3 pods by default
            replicas: 3

            ## Specify a imagePullPolicy
            ## ref: http://kubernetes.io/docs/user-guide/images/#pre-pulling-images
            imagePullPolicy: "IfNotPresent"

            ## Configure resource requests and limits
            ## ref: http://kubernetes.io/docs/user-guide/compute-resources/
            resources: {}
              # limits:
              #   cpu: 200m
              #   memory: 1536Mi
              # requests:
              #   cpu: 100m
              #   memory: 1024Mi

            ## The StatefulSet Update Strategy which Kafka will use when changes are applied.
            ## ref: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#update-strategies
            updateStrategy:
              type: "OnDelete"

            ## The name of the storage class which the cluster should use.
            # storageClass: default

            ## The subpath within the Kafka container's PV where logs will be stored.
            ## This is combined with `persistence.mountPath`, to create, by default: /opt/kafka/data/logs
            logSubPath: "logs"

            ## Use an alternate scheduler, e.g. "stork".
            ## ref: https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/
            ##
            # schedulerName:

            ## Pod scheduling preferences.
            ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
            ##
            affinity: {}

            ## Node labels for pod assignment
            ## ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector
            nodeSelector: {}

            ## Readiness probe config.
            ## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/
            ##
            readinessProbe:
              initialDelaySeconds: 30
              periodSeconds: 10
              timeoutSeconds: 5
              successThreshold: 1
              failureThreshold: 3

            # Tolerations for nodes that have taints on them.
            # Useful if you want to dedicate nodes to just run kafka
            # https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/
            tolerations: []
            # tolerations:
            # - key: "key"
            #   operator: "Equal"
            #   value: "value"
            #   effect: "NoSchedule"

            ## External access.
            ##
            external:
              enabled: false
              servicePort: 19092
              firstListenerPort: 31090
              domain: cluster.local
              init:
                image: "lwolf/kubectl_deployer"
                imageTag: "0.4"
                imagePullPolicy: "IfNotPresent"

            ## Configuration Overrides. Specify any Kafka settings you would like set on the StatefulSet
            ## here in map format, as defined in the official docs.
            ## ref: https://kafka.apache.org/documentation/#brokerconfigs
            ##
            configurationOverrides: {}
              # "offsets.topic.replication.factor": 3
              # "auto.leader.rebalance.enable": true
              # "auto.create.topics.enable": true

              ## Options required for external access via NodePort
              ## ref:
              ## - http://kafka.apache.org/documentation/#security_configbroker
              ## - https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic
              ##
              ## Setting "advertised.listeners" here appends to "PLAINTEXT://${POD_IP}:9092,"
              # "advertised.listeners": |-
              #   EXTERNAL://kafka.cluster.local:$((31090 + ${KAFKA_BROKER_ID}))
              # "listener.security.protocol.map": |-
              #   PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT

            ## A collection of additional ports to expose on brokers (formatted as normal containerPort yaml)
            # Useful when the image exposes metrics (like prometheus, etc.) through a javaagent instead of a sidecar
            additionalPorts: {}

            ## Persistence configuration. Specify if and how to persist data to a persistent volume.
            ##
            persistence:
              enabled: true

              ## The size of the PersistentVolume to allocate to each Kafka Pod in the StatefulSet. For
              ## production servers this number should likely be much larger.
              ##
              size: "80Gi"

              ## The location within the Kafka container where the PV will mount its storage and Kafka will
              ## store its logs.
              ##
              mountPath: "/opt/kafka/data"

              ## Kafka data Persistent Volume Storage Class
              ## If defined, storageClassName: <storageClass>
              ## If set to "-", storageClassName: "", which disables dynamic provisioning
              ## If undefined (the default) or set to null, no storageClassName spec is
              ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
              ##   GKE, AWS & OpenStack)
              ##
              # storageClass:

            ## Metrics
            ##
            metrics:

              jmx:
                enabled: true

                # The image to use for the metrics collector
                image: solsson/kafka-prometheus-jmx-exporter@sha256

                # The image tag to use for the metrics collector
                imageTag: a23062396cd5af1acdf76512632c20ea6be76885dfc20cd9ff40fb23846557e8

                # The port to expose JMX metrics on
                port: 5555

                # Rules to apply to the Kafka JMX Exporter
                kafkaConfig:
                  lowercaseOutputName: true
                  jmxUrl: service:jmx:rmi:///jndi/rmi://127.0.0.1:5555/jmxrmi
                  ssl: false
                  whitelistObjectNames: ["kafka.server:*", "kafka.controller:*", "java.lang:*"]
                  rules:
                    - pattern : kafka.server<type=ReplicaFetcherManager, name=MaxLag, clientId=(.+)><>Value
                    - pattern : kafka.server<type=BrokerTopicMetrics, name=(BytesInPerSec|BytesOutPerSec|MessagesInPerSec), topic=(.+)><>OneMinuteRate
                    - pattern : kafka.server<type=KafkaRequestHandlerPool, name=RequestHandlerAvgIdlePercent><>OneMinuteRate
                    - pattern : kafka.server<type=Produce><>queue-size
                    - pattern : kafka.server<type=ReplicaManager, name=(PartitionCount|UnderReplicatedPartitions)><>(Value|OneMinuteRate)
                    - pattern : kafka.server<type=controller-channel-metrics, broker-id=(.+)><>(.*)
                    - pattern : kafka.server<type=socket-server-metrics, networkProcessor=(.+)><>(.*)
                    - pattern : kafka.server<type=Fetch><>queue-size
                    - pattern : kafka.server<type=SessionExpireListener, name=(.+)><>OneMinuteRate
                    - pattern : kafka.controller<type=KafkaController, name=(.+)><>Value
                    - pattern : java.lang<type=OperatingSystem><>SystemCpuLoad
                    - pattern : java.lang<type=Memory><HeapMemoryUsage>used
                    - pattern : java.lang<type=OperatingSystem><>FreePhysicalMemorySize

              kafka:
                enabled: true

                # The image to use for the metrics collector
                image: danielqsj/kafka-exporter

                # The image tag to use for the metrics collector
                imageTag: v1.0.1
            configurationOverrides: {}
            image:
              tag: sc-cp-kafka:0.0.1-alpha.13
              env:
                KAFKA_HEAP_OPTS:  "-Xmx2g -Xms1g"
                KAFKA_OPTS: "-Dlogging.level=INFO"
                KAFKA_NUM_PARTITIONS: "3"
                KAFKA_DEFAULT_REPLICATION_FACTOR: "3"
                KAFKA_MIN_INSYNC_REPLICAS: "2"
                KAFKA_AUTO_CREATE_TOPICS_ENABLE: "false"
                KAFKA_NUM_RECOVERY_THREADS_PER_DATA_DIR: "1"
                KAFKA_NUM_NETWORK_THREADS: "3"
                KAFKA_NUM_IO_THREADS: "8"
                KAFKA_SOCKET_SEND_BUFFER_BYTES: "1024000"
                KAFKA_SOCKET_RECEIVE_BUFFER_BYTES: "1024000"
                KAFKA_SOCKET_REQUEST_MAX_BYTES: "104857600"
                KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: "2"
                KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: "2"
                KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: "2"
                KAFKA_LOG_RETENTION_HOURS: "60"
                KAFKA_LOG_ROLL_HOURS: "60"
                KAFKA_LOG_RETENTION_BYTES: "1073741824"
                KAFKA_LOG_SEGMENT_BYTES: "1073741824"
                KAFKA_LOG_RETENTION_CHECK_INTERVAL_MS: "300000"
                KAFKA_ZOOKEEPER_CONNECTION_TIMEOUT_MS: "6000"
                KAFKA_DELETE_TOPIC_ENABLE: "true"
                KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: "3"  
              pullPolicy: IfNotPresent
            ## The kafka connect deployment and service
            connect:
              image:
                tag: sc-cp-connect:0.0.1-alpha.13
                env:
                  CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "true"
                  CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "true"
                  CONNECT_CONSUMER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringConsumerInterceptor"
                  CONNECT_PRODUCER_INTERCEPTOR_CLASSES: "io.confluent.monitoring.clients.interceptor.MonitoringProducerInterceptor"
                  CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry.marathon.l4lb.thisdcos.directory:8081"
                  CONNECT_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
                  CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL: "http://schema-registry.marathon.l4lb.thisdcos.directory:8081"
                  CONNECT_PLUGIN_PATH: "/data/kafka/connect/libs"
                  CONNECT_REST_PORT: "8083"
                  CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: "3"
                  CONNECT_GROUP_ID: "sc-connect-group-v3"
                  CONNECT_INTERNAL_KEY_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
                  CONNECT_STATUS_STORAGE_TOPIC: "connect-sc-status"
                  CONNECT_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
                  CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: "3"
                  CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: "3"
                  KAFKA_HEAP_OPTS: "-XX:+PrintGCDetails \
                  -XX:+PrintGCTimeStamps \
                  -XX:+PrintReferenceGC   \
                  -XX:+ParallelRefProcEnabled \
                  -XX:+PrintAdaptiveSizePolicy  \
                  -XX:+UnlockExperimentalVMOptions \
                  -XX:+UseG1GC  \
                  -XX:G1MixedGCLiveThresholdPercent=25 \
                  -XX:InitiatingHeapOccupancyPercent=10 \
                  -Xmx3g"
                  CONNECT_INTERNAL_VALUE_CONVERTER: "org.apache.kafka.connect.json.JsonConverter"
                  CONNECT_OFFSET_STORAGE_TOPIC: "connect-sc-offsets"
                  CONNECT_CONFIG_STORAGE_TOPIC: "connect-sc-configs"
                pullPolicy: IfNotPresent 
              deployment:
                readinessProbe:
                  initialDelaySeconds: 120
                  periodSeconds: 20
                  timeoutSeconds: 10
                containerPort: 8083
                replicas: 3   
                resources:
                  maxMem: "4G"
                  minMem: "1G"
                  maxCpu: "2"
                  minCpu: "0.5"
              service:
                name: kafka-connect
                type: NodePort
                externalPort: 8083
                internalPort: 8083
            rest-proxy:
              image: 
                repository: confluentinc
                tag: cp-kafka-rest:4.0.0
                env:
                pullPolicy: IfNotPresent 
              deployment:
                containerPort: 8082
                replicas: 1   
                resources:
                  maxMem: "768M"
                  minMem: "512M"
              service:
                name: kafka-rest-proxy
                type: NodePort
                externalPort: 8082
                internalPort: 8082
  online:
    sc-rest-layer:
      image:
        tag: sports-cloud-rest:0.0.1-alpha.23
        pullPolicy: IfNotPresent 
        env:
          JAVA_OPTS: "-XX:+PrintGCDetails \
                -XX:+PrintGCTimeStamps \
                -XX:+PrintReferenceGC   \
                -XX:+ParallelRefProcEnabled \
                -XX:+PrintAdaptiveSizePolicy  \
                -XX:+UnlockExperimentalVMOptions \
                -XX:+UseG1GC  \
                -XX:G1MixedGCLiveThresholdPercent=25 \
                -XX:InitiatingHeapOccupancyPercent=10 \
                -XX:MaxMetaspaceSize=256m \
                -Xmx2g"
      deployment:
        containerPort: 9080
        replicas: 3 
        resources:
          maxMem: "3G"
          minMem: "3G"
        livenessProbe:
          initialDelaySeconds: 120
          periodSeconds: 20
          timeoutSeconds: 10
        readinessProbe:
          initialDelaySeconds: 120
          periodSeconds: 20
          timeoutSeconds: 10
      service:
        name: sc-rest
        type: NodePort
        externalPort: 9080
        internalPort: 9080
    artifact-server:
      image:
        tag: artifact-server:0.0.1-alpha.1
        pullPolicy: IfNotPresent 
      deployment:
        containerPort: 9082
        replicas: 1 
        resources:
          maxMem: "256M"
          minMem: "100M"
        livenessProbe:
          initialDelaySeconds: 10
          timeoutSeconds: 5
      service:
        name: artifact-server
        type: NodePort
        clusterIP: 100.68.16.196
        externalPort: 9082
        internalPort: 9082
  tools:
    kubernetes-dashboard:
      # Default values for kubernetes-dashboard
      # This is a YAML-formatted file.
      # Declare name/value pairs to be passed into your templates.
      # name: value

      image: gcr.io/google_containers/kubernetes-dashboard-amd64
      imageTag: "v1.7.1"
      imagePullPolicy: "IfNotPresent"

      nodeSelector: {}

      httpPort: 80

      serviceType: NodePort

      resources:
        limits:
          cpu: 100m
          memory: 50Mi
        requests:
          cpu: 100m
          memory: 50Mi

      ingress:
        ## If true, Kubernetes Dashboard Ingress will be created.
        ##
        enabled: true

        ## Kubernetes Dashboard Ingress annotations
        ##
        annotations:
          alb.ingress.kubernetes.io/scheme: internet-facing
          ingress.kubernetes.io/rewrite-target: /

        ## Kubernetes Dashboard Ingress hostnames
        ## Must be provided if Ingress is enabled
        ##
        hosts:
         - intsportscloud.slingbox.com

        ## Kubernetes Dashboard Ingress TLS configuration
        ## Secrets must be manually created in the namespace
        ##
        # tls:
        #   - secretName: kubernetes-dashboard-tls
        #     hosts:
        #       - kubernetes-dashboard.domain.com

      rbac:
        ## If true, create & use RBAC resources
        #
        create: false

        ## Ignored if rbac.create is true
        #
        serviceAccountName: default
    zeppelin:
      image:
        tag: zeppelin:0.0.1-alpha.6
        pullPolicy: IfNotPresent
        env:
          SPARK_HOME: /opt/spark
          ZEPPELIN_HOME: /opt/zeppelin
          ZEPPELIN_JAVA_OPTS: "-Dspark.jars: /opt/spark/jars/gcs-connector-latest-hadoop2.jar"
          CLASSPATH: "/opt/spark/lib/gcs-connector-latest-hadoop2.jar"
          ZEPPELIN_NOTEBOOK_DIR: "/opt/zeppelin/notebook"
          ZEPPELIN_MEM: -Xmx3g
          ZEPPELIN_PORT: 8080
          PYTHONPATH: "/opt/spark/python:/opt/spark/python/lib/py4j-0.8.2.1-src.zip"
          ZEPPELIN_CONF_DIR: "/opt/zeppelin/conf"
          APP_SPARK_MASTERS_EPS: "local[3]"
      deployment:
        command:
        - /opt/zeppelin/bin/docker-zeppelin.sh
        resources:
          minCpu: 2
          maxCpu: 3
          minMem: 2G
          maxMem: 3G
        containerPort: 8080
        replicas: 1   
      service:
        name: zeppelin
        type: NodePort
        externalPort: 8080
        internalPort: 80
      spark:
        driverMemory: 2g
        executorMemory: 2g
        numExecutors: 2
        coresMax: 3
    kibana:
      image:
        repository: "docker.elastic.co/kibana"
        tag: "kibana-oss:6.0.0"
        pullPolicy: IfNotPresent   
        env:
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 1000m
        containerPort: 5601
        replicas: 1   
      service:
        name: kibana
        type: NodePort
        externalPort: 5601
        internalPort: 80
      ingress:
        ## If true, Kubernetes Dashboard Ingress will be created.
        ##
        enabled: true

        ## Kubernetes Dashboard Ingress annotations
        ##
        annotations:
          alb.ingress.kubernetes.io/scheme: internet-facing
          ingress.kubernetes.io/rewrite-target: /

        ## Kubernetes Dashboard Ingress hostnames
        ## Must be provided if Ingress is enabled
        ##
        hosts:
         - intsc-kibana.slingbox.com
    spark-ui-proxy-1:
      image:
        repository: elsonrodriguez
        tag: spark-ui-proxy:1.0
        pullPolicy: IfNotPresent   
      deployment:
        resources:
          cpu: 100m
        containerPort: 80
        replicas: 1   
        args: 
        - "80"
        livenessProbeInitDelay: 120
        livenessProbeTimeoutSeconds: 5
        livenessProbePort: 80  
      service:
        name: spark-ui-proxy-1
        type: NodePort
        externalPort: 80
        internalPort: 80
    spark-ui-proxy-2:
      image:
        repository: elsonrodriguez
        tag: spark-ui-proxy:1.0
        pullPolicy: IfNotPresent   
      deployment:
        resources:
          cpu: 100m
        containerPort: 80
        replicas: 1   
        args: 
        - "80"
        livenessProbeInitDelay: 120
        livenessProbeTimeoutSeconds: 5
        livenessProbePort: 80  
      service:
        name: spark-ui-proxy-2
        type: NodePort
        externalPort: 80
        internalPort: 80
sc-jobs:
  thuuz-download-job:
    ingress: {}
    service: {}
    image:
      tag: sc-job-scheduler:0.0.1-alpha.13
      pullPolicy: IfNotPresent
      env:
        cmsHost: "cbd46b77"
        cmsSummaryUrl: "cms/publish3/domain/summary/1.json"
    cronJob:
      args: 
      - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
      - "download-thuuz"
      type: ClusterIP
      restartPolicy: Never
      schedule: "0 */5 * * *"
  summary-download-job:
    ingress: {}
    service: {}
    deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 256Mi
          maxMem: 512Mi
    image:
      tag: sc-job-scheduler:0.0.1-alpha.13
      pullPolicy: IfNotPresent
      env:
        cmsHost: "cbd46b77"
        cmsSummaryUrl: "cms/publish3/domain/summary/1.json"
        JAVA_OPTS: "-Xmx512M"
    cronJob:
      args: 
      - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
      - "download-summary"
      type: ClusterIP
      restartPolicy: Never
      schedule: "20 0/5 * * *"
  schedules-download-job:
    ingress: {}
    service: {}
    deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 256Mi
          maxMem: 512Mi
    image:
      tag: sc-job-scheduler:0.0.1-alpha.13
      pullPolicy: IfNotPresent
      env:
        cmsHost: "cbd46b77"
        cmsSummaryUrl: "cms/publish3/domain/summary/1.json"
        JAVA_OPTS: "-Xmx512M"
    cronJob:
      args: 
      - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
      - "download-schedules"
      type: ClusterIP
      restartPolicy: Never
      schedule: "40 0/5 * * *"
  nfl:    
    connect-batch-job-nfl:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectBatchJobs"
        - "nfl"
        - "900000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    connect-live-info-nfl:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectLiveInfoJob"
        - "nfl"
        - "180000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    spark-meta-batch-nfl:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: NflTeamStandingsMetaDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "TEAMSTANDINGS"
        - "meta_batch_nfl"
        - "team_standings"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "20 5 * * *"
    spark-content-match-nfl:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.impl.ContentMatcher
        jobName: NflContentDataMuncher
        driverMem: 3G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "content_match_nfl"
        - "game_schedule"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "40 8 * * *"
    spark-live-info-nfl:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.streaming.live.impl.NflLiveDataMuncher
        jobName: NflLiveDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "live_info_nfl"
        - "live_info"
      Job:
        replicas: 1        
        command: 
        type: ClusterIP
        restartPolicy: OnFailure
    spark-batch-live-score-nfl:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: NflBoxScore
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "NFLLIVEINFO"
        - "live_info_nfl"
        - "live_info"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "0 8 * * *"
  mlb:    
    connect-batch-job-mlb:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectBatchJobs"
        - "mlb"
        - "900000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    connect-live-info-mlb:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectLiveInfoJob"
        - "mlb"
        - "180000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    spark-meta-batch-ts-mlb:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: MlbTeamStandingsMetaDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "TEAMSTANDINGS"
        - "meta_batch_mlb"
        - "team_standings"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "20 5 * * *"
    spark-meta-batch-ps-mlb:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: MlbPlayerStatsMetaDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "PLAYERSTATS"
        - "meta_batch_mlb"
        - "player_stats"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "15 * * * *"
    spark-content-match-mlb:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.impl.ContentMatcher
        jobName: MlbContentDataMuncher
        driverMem: 3G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "content_match_mlb"
        - "game_schedule"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "40 8 * * *"
    spark-live-info-mlb:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.streaming.live.impl.MlbLiveDataMuncher
        jobName: MlbLiveDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "live_info_mlb"
        - "live_info"
      Job:
        replicas: 1
        command: 
        type: ClusterIP
        restartPolicy: OnFailure
    spark-batch-live-score-mlb:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: MlbBoxScore
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "LIVEINFO"
        - "live_info_mlb"
        - "live_info"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "5 8 * * *"
  ncaaf:    
    connect-batch-job-ncaaf:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectBatchJobs"
        - "ncaaf"
        - "900000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    connect-live-info-ncaaf:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectLiveInfoJob"
        - "ncaaf"
        - "180000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    spark-meta-batch-ncaaf:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: NcaafTeamStandingsMetaDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "TEAMSTANDINGS"
        - "meta_batch_ncaaf"
        - "team_standings"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "20 5 * * *"
    spark-content-match-ncaaf:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.impl.ContentMatcher
        jobName: NcaafContentDataMuncher
        driverMem: 3G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "content_match_ncaaf"
        - "game_schedule"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "40 8 * * *"
    spark-live-info-ncaaf:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.streaming.live.impl.NflLiveDataMuncher
        jobName: NcaafLiveDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "live_info_ncaaf"
        - "live_info"
      Job:
        replicas: 1        
        command: 
        type: ClusterIP
        restartPolicy: OnFailure
        schedule: "30 7 * * *"
    spark-batch-live-score-ncaaf:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: NcaafBoxScore
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "NCAAFLIVEINFO"
        - "live_info_ncaaf"
        - "live_info"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "3 8 * * *"
  nba:
    connect-batch-job-nba:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectBatchJobs"
        - "nba"
        - "900000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    connect-live-info-nba:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectLiveInfoJob"
        - "nba"
        - "180000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    connect-player-game-stats-nba:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectPlayerGameStatsJob"
        - "nba"
        - "180000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    spark-meta-batch-ts-nba:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: NbaTeamStandingsMetaDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "TEAMSTANDINGS"
        - "meta_batch_nba"
        - "team_standings"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "20 5 * * *"
    spark-meta-batch-pstats-nba:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.nba.NbaMetaDataMuncher
        jobName: NbaPlayerStatsMetaDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "PLAYERSTATS"
        - "meta_batch_nba"
        - "player_stats"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "40 5 * * *"
    spark-content-match-nba:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.impl.ContentMatcher
        jobName: NbaContentDataMuncher
        driverMem: 3G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "content_match_nba"
        - "game_schedule"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "40 8 * * *"
    spark-live-info-nba:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.streaming.live.impl.NbaLiveDataMuncher
        jobName: NbaLiveDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "live_info_nba"
        - "live_info"
      Job:
        replicas: 1        
        command: 
        type: ClusterIP
        restartPolicy: OnFailure
        schedule: "30 7 * * *"
    spark-batch-live-score-nba:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: NbaBoxScore
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "NBALIVEINFO"
        - "live_info_nba"
        - "live_info"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "5 8 * * *"
    spark-player-game-stats-nba:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.streaming.playergamestats.impl.NbaPlayerGameStatsMuncher
        jobName: NbaPlayerGameStatsMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "player_game_stats_nba"
        - "player_game_stats"
      Job:
        replicas: 1
        command:
        type: ClusterIP
        restartPolicy: OnFailure
        schedule: "30 7 * * *"
  ncaab:
    connect-batch-job-ncaab:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectBatchJobs"
        - "ncaab"
        - "900000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    connect-live-info-ncaab:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectLiveInfoJob"
        - "ncaab"
        - "180000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    connect-player-game-stats-ncaab:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command:
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectPlayerGameStatsJob"
        - "ncaab"
        - "180000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    spark-meta-batch-ts-ncaab:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: NcaabTeamStandingsMetaDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "TEAMSTANDINGS"
        - "meta_batch_ncaab"
        - "team_standings"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "20 5 * * *"
    spark-meta-batch-pstats-ncaab:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: NcaabPlayerStatsMetaDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "PLAYERSTATS"
        - "meta_batch_ncaab"
        - "player_stats"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "40 5 * * *"
    spark-content-match-ncaab:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.impl.ContentMatcher
        jobName: NcaabContentDataMuncher
        driverMem: 3G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "content_match_ncaab"
        - "game_schedule"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "40 8 * * *"
    spark-live-info-ncaab:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.streaming.live.impl.NbaLiveDataMuncher
        jobName: NcaabLiveDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "live_info_ncaab"
        - "live_info"
      Job:
        replicas: 1        
        command: 
        type: ClusterIP
        restartPolicy: OnFailure
        schedule: "30 7 * * *"
    spark-batch-live-score-ncaab:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: NcaabBoxScore
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "NCAABLIVEINFO"
        - "live_info_ncaab"
        - "live_info"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "5 8 * * *"
    spark-player-game-stats-ncaab:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.streaming.playergamestats.impl.NbaPlayerGameStatsMuncher
        jobName: NcaabPlayerGameStatsMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "player_game_stats_ncaab"
        - "player_game_stats"
      Job:
        replicas: 1
        command:
        type: ClusterIP
        restartPolicy: OnFailure
        schedule: "30 7 * * *"
  soccer:
    spark-content-match-soccer:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.20
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.impl.leagues.SoccerContentMatcher
        jobName: SoccerContentDataMuncher
        driverMem: 3G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "content_match_soccer"
        - "game_schedule"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "40 0/8 * * *"
    connect-live-info-soccer:
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "rest-parser-jobs"
        - "com.slingmedia.sportscloud.rest.parsers.leagues.soccer.SoccerLiveParser"
        - "live_info_soccer"
    spark-live-info-soccer:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.20
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.streaming.live.impl.SoccerLiveDataMuncher
        jobName: SoccerLiveDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "live_info_soccer"
        - "live_info"
      Job:
        replicas: 1        
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "30 7 * * *"
  nhl:
    connect-batch-job-nhl:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectBatchJobs"
        - "nhl"
        - "900000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    connect-live-info-nhl:
      image:
        tag: sc-job-scheduler:0.0.1-alpha.13
        pullPolicy: IfNotPresent
        env:
      Job:
        replicas: 1
        command: 
        - "/deploy-scheduled-jobs/scheduled-job-driver.sh"
        - "connect-jobs"
        - "com.slingmedia.sportscloud.schedulers.KafkaConnectLiveInfoJob"
        - "nhl"
        - "180000"
      deployment:
        resources:
          minCpu: 100m
          maxCpu: 100m
          minMem: 64Mi
          maxMem: 64Mi
    spark-meta-batch-ts-nhl:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: NhlTeamStandingsMetaDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "TEAMSTANDINGS"
        - "meta_batch_nhl"
        - "team_standings"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "20 5 * * *"
    spark-meta-batch-pstats-nhl:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.meta.impl.DefaultMetaDataMuncher
        jobName: NhlPlayerStatsMetaDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "PLAYERSTATS"
        - "meta_batch_nhl"
        - "player_stats"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "40 5 * * *"
    spark-content-match-nhl:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.batch.impl.ContentMatcher
        jobName: NhlContentDataMuncher
        driverMem: 3G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "content_match_nhl"
        - "game_schedule"
      cronJob:
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "40 8 * * *"
    spark-live-info-nhl:
      ingress: {}
      service: {}
      image:
        tag: spark-job:0.0.1-alpha.17
        pullPolicy: IfNotPresent
        env:
      sparkJobArgs:
        className: com.slingmedia.sportscloud.offline.streaming.live.impl.NbaLiveDataMuncher
        jobName: NhlLiveDataMuncher
        driverMem: 2G
        executorMem: 2G
        executorCores: 2
        parallelism: 4
        execArgs:
        - "live_info_nhl"
        - "live_info"
      Job:
        replicas: 1        
        command: 
        type: ClusterIP
        restartPolicy: Never
        schedule: "30 7 * * *"

